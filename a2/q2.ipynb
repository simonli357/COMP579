{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Q2: Policy Evaluation (π(s1)=a1, π(s2)=a1) ===\n",
      "Analytical solution:\n",
      "  v(s1) = 2.9000, v(s2) = 1.0000, v(s3) = 0\n",
      "Iterative solution:\n",
      "  v(s1) = 2.9000, v(s2) = 1.0000, v(s3) = 0\n",
      "\n",
      "=== Q3: Optimal Value Function (Analytical) ===\n",
      "Optimal value function:\n",
      "  v*(s1) = 5.78950, v*(s2) = 4.21055, v*(s3) = 0\n",
      "Optimal policy (analytical):\n",
      "  π*(s1) = a1   (since 2+0.9*v*(s2) ≈ 5.7895 > 5)\n",
      "  π*(s2) = a2   (since -1+0.9*v*(s1) ≈ 4.21055 > 1)\n",
      "\n",
      "=== Q3: Optimal Value Function (Value Iteration) ===\n",
      "Optimal value function from value iteration:\n",
      "  v*(s1) = 5.78947, v*(s2) = 4.21052, v*(s3) = 0\n",
      "Optimal policy from value iteration:\n",
      "  π*(s1) = a1\n",
      "  π*(s2) = a2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "#############################\n",
    "# MDP Definition\n",
    "#############################\n",
    "# States: s1, s2, s3 (we’ll index them as 0, 1, 2)\n",
    "# Terminal state: s3 (index 2) with v(s3)=0.\n",
    "# Actions available in s1 and s2: a1 and a2.\n",
    "#\n",
    "# Transitions and rewards:\n",
    "# For s1 (index 0):\n",
    "#   - a1: deterministically go to s2 (index 1) with reward 2.\n",
    "#   - a2: deterministically go to s3 (index 2) with reward 5.\n",
    "#\n",
    "# For s2 (index 1):\n",
    "#   - a1: deterministically go to s3 (index 2) with reward 1.\n",
    "#   - a2: deterministically go to s1 (index 0) with reward -1.\n",
    "#\n",
    "# Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "######################################\n",
    "# Q2: Policy Evaluation for a given π\n",
    "######################################\n",
    "# Given deterministic policy π:\n",
    "#   π(s1) = a1\n",
    "#   π(s2) = a1\n",
    "#\n",
    "# Under this policy, only the following transitions occur:\n",
    "#   s1 --[a1]--> s2 with reward 2, and\n",
    "#   s2 --[a1]--> s3 with reward 1.\n",
    "#\n",
    "# (a) Analytical solution using matrix inversion:\n",
    "#\n",
    "# For nonterminal states s1 and s2, the Bellman equations are:\n",
    "#   v(s1) = 2 + gamma * v(s2)\n",
    "#   v(s2) = 1 + gamma * v(s3) = 1   (since v(s3)=0)\n",
    "#\n",
    "# In matrix form (for v = [v(s1), v(s2)]):\n",
    "#   [1    -gamma] [v(s1)]   = [2]\n",
    "#   [0      1   ] [v(s2)]     [1]\n",
    "A = np.array([[1, -gamma],\n",
    "              [0, 1]])\n",
    "b = np.array([2, 1])\n",
    "v_policy_analytical = np.linalg.solve(A, b)\n",
    "v1_pi_analytical = v_policy_analytical[0]\n",
    "v2_pi_analytical = v_policy_analytical[1]\n",
    "v3_pi_analytical = 0  # Terminal\n",
    "\n",
    "# (b) Numerical solution by iterative updates:\n",
    "v_policy_iter = np.zeros(3)  # [v(s1), v(s2), v(s3)]\n",
    "tol = 1e-6\n",
    "while True:\n",
    "    v_new = np.copy(v_policy_iter)\n",
    "    # For s1: only action a1 is taken, leading to s2.\n",
    "    v_new[0] = 2 + gamma * v_policy_iter[1]\n",
    "    # For s2: under a1, it goes to terminal s3.\n",
    "    v_new[1] = 1  # since 1 + gamma * 0 = 1\n",
    "    # s3 remains terminal with value 0.\n",
    "    if np.max(np.abs(v_new - v_policy_iter)) < tol:\n",
    "        break\n",
    "    v_policy_iter = v_new\n",
    "\n",
    "v1_pi_iter, v2_pi_iter, v3_pi_iter = v_policy_iter\n",
    "\n",
    "print(\"=== Q2: Policy Evaluation (π(s1)=a1, π(s2)=a1) ===\")\n",
    "print(\"Analytical solution:\")\n",
    "print(\"  v(s1) = {:.4f}, v(s2) = {:.4f}, v(s3) = 0\".format(v1_pi_analytical, v2_pi_analytical))\n",
    "print(\"Iterative solution:\")\n",
    "print(\"  v(s1) = {:.4f}, v(s2) = {:.4f}, v(s3) = 0\".format(v1_pi_iter, v2_pi_iter))\n",
    "\n",
    "\n",
    "##############################################\n",
    "# Q3: Finding the Optimal Policy and Value v*\n",
    "##############################################\n",
    "#\n",
    "# The Bellman optimality equations for each nonterminal state are:\n",
    "#\n",
    "# For s1:\n",
    "#   v*(s1) = max { \n",
    "#         a1: 2 + gamma*v*(s2), \n",
    "#         a2: 5 + gamma*v*(s3) } = max { 2 + 0.9*v*(s2), 5 }\n",
    "#\n",
    "# For s2:\n",
    "#   v*(s2) = max {\n",
    "#         a1: 1 + gamma*v*(s3), \n",
    "#         a2: -1 + gamma*v*(s1) } = max { 1, -1 + 0.9*v*(s1) }\n",
    "#\n",
    "# And v*(s3)=0.\n",
    "#\n",
    "# (a) Analytical solution:\n",
    "# We need to choose the actions that maximize the right-hand sides.\n",
    "# Let’s denote:\n",
    "#   x = v*(s1) and y = v*(s2)\n",
    "#\n",
    "# Suppose the optimal action in s1 is a1 and in s2 is a2.\n",
    "# Then:\n",
    "#   x = 2 + 0.9*y\n",
    "#   y = -1 + 0.9*x\n",
    "#\n",
    "# Substitute y from the second into the first:\n",
    "#   x = 2 + 0.9*(-1 + 0.9*x) = 2 - 0.9 + 0.81*x\n",
    "#   => x - 0.81*x = 1.1   => 0.19*x = 1.1   => x ≈ 5.7895\n",
    "# Then:\n",
    "#   y = -1 + 0.9*5.7895 ≈ -1 + 5.21055 ≈ 4.21055\n",
    "#\n",
    "# We verify the choices:\n",
    "# For s1: a1 yields 2+0.9*y ≈ 2+3.7895=5.7895; a2 yields 5.\n",
    "# Hence, a1 is optimal since 5.7895 > 5.\n",
    "#\n",
    "# For s2: a1 yields 1; a2 yields -1+0.9*x ≈ -1+5.21055=4.21055.\n",
    "# Hence, a2 is optimal since 4.21055 > 1.\n",
    "#\n",
    "v1_opt_analytical = 5.7895\n",
    "v2_opt_analytical = 4.21055\n",
    "v3_opt_analytical = 0\n",
    "\n",
    "print(\"\\n=== Q3: Optimal Value Function (Analytical) ===\")\n",
    "print(\"Optimal value function:\")\n",
    "print(\"  v*(s1) = {:.5f}, v*(s2) = {:.5f}, v*(s3) = 0\".format(v1_opt_analytical, v2_opt_analytical))\n",
    "print(\"Optimal policy (analytical):\")\n",
    "print(\"  π*(s1) = a1   (since 2+0.9*v*(s2) ≈ 5.7895 > 5)\")\n",
    "print(\"  π*(s2) = a2   (since -1+0.9*v*(s1) ≈ 4.21055 > 1)\")\n",
    "\n",
    "# (b) Numerical solution via value iteration:\n",
    "v_opt = np.zeros(3)\n",
    "while True:\n",
    "    v_new = np.copy(v_opt)\n",
    "    # Terminal state remains 0:\n",
    "    v_new[2] = 0\n",
    "    # For s1:\n",
    "    #   a1: 2 + gamma*v(s2)\n",
    "    #   a2: 5 + gamma*v(s3) = 5\n",
    "    val_a1 = 2 + gamma * v_opt[1]\n",
    "    val_a2 = 5\n",
    "    v_new[0] = max(val_a1, val_a2)\n",
    "    # For s2:\n",
    "    #   a1: 1 + gamma*v(s3) = 1\n",
    "    #   a2: -1 + gamma*v(s1)\n",
    "    val_a1_s2 = 1\n",
    "    val_a2_s2 = -1 + gamma * v_opt[0]\n",
    "    v_new[1] = max(val_a1_s2, val_a2_s2)\n",
    "    if np.max(np.abs(v_new - v_opt)) < tol:\n",
    "        break\n",
    "    v_opt = v_new\n",
    "\n",
    "v1_opt_vi, v2_opt_vi, v3_opt_vi = v_opt\n",
    "\n",
    "# Derive the optimal policy from the converged value function:\n",
    "# For s1:\n",
    "if (2 + gamma * v_opt[1]) >= 5:\n",
    "    policy_s1 = 'a1'\n",
    "else:\n",
    "    policy_s1 = 'a2'\n",
    "# For s2:\n",
    "if (1) >= (-1 + gamma * v_opt[0]):\n",
    "    policy_s2 = 'a1'\n",
    "else:\n",
    "    policy_s2 = 'a2'\n",
    "\n",
    "print(\"\\n=== Q3: Optimal Value Function (Value Iteration) ===\")\n",
    "print(\"Optimal value function from value iteration:\")\n",
    "print(\"  v*(s1) = {:.5f}, v*(s2) = {:.5f}, v*(s3) = 0\".format(v1_opt_vi, v2_opt_vi))\n",
    "print(\"Optimal policy from value iteration:\")\n",
    "print(\"  π*(s1) = {}\".format(policy_s1))\n",
    "print(\"  π*(s2) = {}\".format(policy_s2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
