\documentclass{article}
\usepackage[final]{neurips_2024}
\usepackage[T1]{fontenc}    
\usepackage{hyperref}       
\usepackage{url}            
\usepackage{booktabs}       
\usepackage{amsfonts}       
\usepackage{nicefrac}       
\usepackage{microtype}      
\usepackage{xcolor}         
\usepackage{graphicx}   
\usepackage{amsmath}
\usepackage{float}

\setcitestyle{numbers}

\title{COMP 579 Final Project Report: Reinforcement Learning for Ms. Pac-Man}

\author{
    Ivy Hu\quad
    Simon Li\quad
    Kenza Bellebouir \\
    \texttt{nanqing.hu@mail.mcgill.ca} \quad
    \texttt{xi.yang.li@mcgill.ca} \quad
    \texttt{kenza.bellebouir@mail.mcgill.ca}
}

\begin{document}

\maketitle

\begin{abstract}
  We compare Rainbow DQN and Proximal Policy Optimization (PPO) in the visually complex Ms. Pac-Man environment. Rainbow DQN achieves higher peak rewards and stronger long-term learning, while PPO converges faster early on but suffers from instability. We further conduct ablation and color perturbation studies to assess architectural contributions and generalization, offering practical insights into RL training under sparse rewards and visual complexity.
\end{abstract}

\section{Introduction}

Deep reinforcement learning (RL) has achieved notable success in high-dimensional decision-making tasks, particularly within the Arcade Learning Environment (ALE) \cite{ale}, which provides a unified platform for evaluating algorithms on Atari 2600 games \cite{mnih2015human}. These environments are characterized by partial observability, sparse and delayed rewards, and the need for both short-term reactivity and long-term planning.

Among them, \textit{Ms. Pac-Man} is a particularly challenging domain due to its stochastic dynamics, diverse strategic possibilities, and visually rich inputs. It is thus well-suited for evaluating algorithmic robustness and scalability. In this work, we compare two prominent deep RL methods: Rainbow DQN \cite{hessel2018rainbow}, an enhanced value-based agent that integrates six key improvements over DQN, and Proximal Policy Optimization (PPO) \cite{schulman2017proximal}, a widely used policy-gradient algorithm known for its simplicity.

We implement both agents in the \texttt{MsPacman-v0} environment and conduct a detailed empirical evaluation. Beyond comparing baseline performance, we perform an ablation study of Rainbow DQN to assess the contribution of prioritized experience replay, noisy networks, and multi-step returns. To evaluate generalization, we test trained agents under visual domain shifts introduced via color perturbations. We further analyze sensitivity to hyperparameters by conducting targeted sweeps to identify configurations yielding strong and stable performance.

This study provides insights into the design and evaluation of RL in complex visual settings, highlighting the internal dynamics and generalization capabilities of Rainbow DQN relative to PPO.

\section{Background}

\subsection{Reinforcement Learning and MDPs}

Reinforcement Learning formalizes sequential decision-making as a Markov Decision Process (MDP), defined by $(\mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma)$, where $\mathcal{S}$ is the state space, $\mathcal{A}$ the action space, $\mathcal{P}(s'|s,a)$ the transition model, $\mathcal{R}(s,a)$ the reward function, and $\gamma \in [0,1)$ the discount factor. The agent selects actions to maximize expected return $\mathbb{E}[\sum_t \gamma^t r_t]$ based on its interactions with the environment.

\subsection{Rainbow DQN}

Rainbow DQN \cite{hessel2018rainbow} combines several enhancements to the original DQN \cite{mnih2015human}, aiming to improve sample efficiency, stability, and exploration. Double Q-learning addresses overestimation bias by decoupling action selection and evaluation in target computation. The dueling network architecture separates state-value and advantage estimation to refine action selection. Prioritized experience replay focuses learning on high-error transitions, improving data efficiency. Noisy networks inject trainable stochasticity into parameters, facilitating more efficient exploration. Multi-step returns accelerate learning by propagating rewards over multiple future steps. Distributional RL (C51) models a categorical distribution over future returns, enabling richer value representation.

\subsection{Proximal Policy Optimization (PPO)}

PPO \cite{schulman2017proximal} is a first-order policy-gradient algorithm that improves training stability by limiting policy updates through a clipped surrogate objective:
\[
L^{\text{CLIP}}(\theta) = \mathbb{E}_t \left[ \min \left( r_t(\theta) \hat{A}_t, \text{clip}(r_t(\theta), 1 - \epsilon, 1 + \epsilon) \hat{A}_t \right) \right],
\]
where $r_t(\theta)$ is the probability ratio between the new and old policies, and $\hat{A}_t$ is an estimate of the advantage function. PPO strikes a balance between exploration and stability, making it suitable for a wide range of tasks without extensive tuning.

\subsection{Related Work}
Past approaches to Ms. Pac-Man include recurrent architectures for partial observability \cite{toromanoff2019deep} and planning-based methods like Monte Carlo Tree Search \cite{pieters2016monte}, often relying on engineered features. In contrast, we adopt a fully pixel-based learning setup and extend the robustness analysis via controlled visual perturbations.

\section{Methodology}

We evaluate agents in the \texttt{MsPacman-v0} environment from OpenAI Gym, based on the Arcade Learning Environment (ALE) \cite{ale}. The environment outputs raw $210 \times 160 \times 3$ RGB frames, which we preprocess into $84 \times 84$ grayscale or color-normalized images scaled to $[0, 1]$. To encode temporal dynamics, we stack the last 4 frames as input. The action space comprises 9 discrete actions: four cardinal, four diagonal directions, and a no-op. Rewards are sparse: +10 for pellets, +200 for ghosts after a power pellet, and –1 per time step. Episodes end after the agent loses all 3 lives.

We employ two preprocessing pipelines. The baseline pipeline converts RGB frames to grayscale, resizes them to $84 \times 84$ via bilinear interpolation, normalizes the pixel values, and stacks four frames. To evaluate robustness, we also introduce a \texttt{ColorPreprocessFrame} wrapper that applies red, green, or blue tints or OpenCV colormaps (e.g., \texttt{cv2.COLORMAP\_JET}) to grayscale frames, yielding semantically identical but visually altered 3-channel RGB inputs.

Our Rainbow DQN implementation follows the full architecture described in \cite{hessel2018rainbow}. It consists of a convolutional encoder (32, 64, 64 filters), a dueling head for value and advantage estimation, and NoisyLinear layers with initial standard deviation $\sigma=0.017$. The output is distributional, with 51 atoms over $[-10, 10]$. We use prioritized experience replay ($\alpha = 0.6$, $\beta$ annealed from 0.4 to 1.0) and multi-step returns with $n = 3$. Target networks are updated every 1,000 steps. The model is trained using Adam with a learning rate of $1 \times 10^{-4}$.

PPO follows a shared convolutional encoder (identical to Rainbow), feeding into a 512-unit fully connected layer, then branching into actor and critic heads. We optimize the clipped surrogate objective ($\epsilon$) and use GAE with $\lambda$. PPO is trained with 3 different sets of hyperparameters representing different scenarios to determine which would be able to perform best.

All agents are trained using stacked-frame observations and the Adam optimizer. Rainbow DQN is trained for 30,000–50,000 frames per setting; PPO is trained for 1,000 steps due to its on-policy nature. We use a fixed seed (357) across all experiments. Evaluation is performed post-training, with exploration disabled. Ablation and color robustness studies were run on an NVIDIA RTX 3080 laptop, while hyperparameter tuning was performed on a Mac notebook. Training time ranged from 2 to 5 hours depending on the setup.

To examine hyperparameter sensitivity, we ran a small grid search on three Rainbow DQN parameters: the Adam learning rate $\eta$, multi-step return length $n$, and prioritization exponent $\alpha$. Other settings remained fixed ($\gamma=0.99$, buffer size $10^5$, batch size 32, target network update every 1,000 frames, and $\beta$ annealed over 100,000 steps). We evaluated the cross-product:
\[
\eta \in \{1\times10^{-5},\,5\times10^{-5}\},\quad
n \in \{3,\,5\},\quad
\alpha \in \{0.4,\,0.6\},
\]
yielding eight configurations. Each was trained for 30,000 frames using the same seed. This sweep helped identify settings that enhanced reward growth or contributed to performance variance.

\section{Results and Discussion}

\subsection{PPO Hyperparameter Sensitivity}

We evaluated three PPO configurations: \textit{Balanced} (blue), \textit{Fast Training} (orange), and \textit{Aggressive} (green). The Aggressive setup, combining long rollouts, tight gradient clipping, and low entropy, consistently achieved the highest rewards (approaching 1,000), demonstrating a favorable trade-off between stability and efficiency. In contrast, Fast Training plateaued early at lower scores, while Balanced learning remained steady but suboptimal. Results were reproducible across three runs for each configuration.

\subsection{Comparative Performance: Rainbow DQN vs. PPO}

Figure~\ref{fig:ppo} and Figure~\ref{fig:rainbow} compares PPO and Rainbow DQN learning in \texttt{MsPacman-v0}. PPO displays early convergence to a moderate range (600–800 points) but deteriorates after update 800, likely due to sampling noise and hyperparameter sensitivity. It still manages to acquire basic survival and collection behaviors.

Rainbow DQN, though more volatile, shows a clearer upward trajectory with multiple peaks surpassing 1,000 and 2,000 points. These spikes indicate discovery of higher-reward strategies, benefiting from Rainbow’s architectural features like prioritized replay and distributional outputs. Despite its instability, Rainbow demonstrates superior long-term reward potential within the same frame budget.

\subsection{Insights and Interpretation}

Rainbow DQN consistently outperforms PPO in peak performance, affirming the effectiveness of value-based methods enhanced with multi-step returns, noisy exploration, and replay prioritization in sparse-reward, long-horizon tasks. However, high variance in both methods suggests neither has fully stabilized. PPO’s reward collapse highlights the challenges of on-policy learning, while Rainbow’s fluctuations point to sensitivity in replay dynamics and learning rate tuning. Rainbow’s stronger trend suggests greater promise under the current constraints, while PPO may benefit from longer rollouts or adjusted GAE parameters.

\subsection{Rainbow DQN Hyperparameter Ablation}

Figure~\ref{fig:ablation} presents eight Rainbow DQN configurations varying in learning rate \(\eta\), multi-step return \(n\), and prioritization strength \(\alpha\). Four settings stand out: D, E, G, and H — all using \(\eta = 5 \times 10^{-5}\) or compensating via increased \(\alpha\), and three with \(n = 5\). These combinations yielded higher final rewards, suggesting that deeper bootstrapping and moderately aggressive learning rates facilitate better performance in sparse-reward environments.

\subsection{Extended Sweep: Replay and Update Scaling}

To test broader settings, we re-ran all eight ablations with a larger replay buffer (1M), slower target updates (every 5,000 steps), and extended \(\beta\)-annealing (500K frames). As shown in Figure~\ref{fig:extended_sweep}, all configurations improved relative to their 30K-frame baselines, with C\_large, E\_large, G\_large, and H\_large showing the greatest gains. These settings share a larger buffer and, in most cases, a 5-step return.

Despite improvements, all runs remained highly variable—fluctuating by \(\pm500\) points per episode. This reflects the stochastic nature of prioritized replay and the challenge of sparse rewards in Ms. Pac-Man. While extended settings enhanced median returns, further progress may require additional seeds, longer training durations, or targeted variance-reduction techniques.

% \subsection{Seed‐Level Reproducibility on Rainbow DQN}

% To assess the stability of our hyperparameter choices, we re‑ran Configurations A and E across three different random seeds (\(0,1,2\)). Figures~\ref{fig:seed_A} and \ref{fig:seed_E} show, for each seed, the episode‑wise total reward (dashed lines), the mean curve (solid black), and the shaded \(\pm1\sigma\) band.

% Overall, Configuration E achieves a higher average reward than A, but at the cost of increased jitter. These results underscore the importance of reporting across multiple seeds—averaging over 3–5 runs provides a more reliable estimate of true performance than any single trial. Given the wide per seed variability, reporting a single run can be misleading. Averaging across 3–5 seeds provides a more robust estimate of true algorithmic performance.


\subsection{Rainbow DQN Ablation Study}

To elucidate the contributions of individual components in the Rainbow DQN architecture, we conducted an ablation study comparing three simplified variants: \textbf{nstep1}, with multi-step returns disabled by setting $n=1$; \textbf{no\_prior}, using uniform sampling in place of prioritized replay; and \textbf{no\_noisy}, where NoisyLinear layers are replaced with standard linear ones, disabling learned exploration.

Figure~\ref{fig:rainbow_ablation} presents training performance across 45 episodes. All variants exhibit similar average scores (200–700 points), yet distinct patterns emerge.

The \textbf{full Rainbow} agent occasionally achieves notably higher rewards—e.g., sharp peaks around episodes 7 and 20—implying that the integration of prioritized replay, multi-step returns, and noisy exploration enhances strategic discovery. Nonetheless, its performance remains volatile in early training, suggesting instability.

The \textbf{no\_prior} agent performs competitively in later stages, at times surpassing full Rainbow after episode 25. This implies that prioritized replay, while beneficial for early learning, may not be crucial for final policy quality under constrained training.

The \textbf{no\_noisy} variant shows pronounced late-stage gains, exceeding 1000 points in several episodes. This suggests that deterministic policies may yield greater stability in this domain, albeit with less effective initial exploration.

By contrast, the \textbf{nstep1} configuration consistently underperforms, underscoring the critical role of multi-step bootstrapping in environments with sparse, delayed rewards such as Ms. Pac-Man.

To evaluate stability, we re-ran key configurations (e.g., A, E) across multiple seeds. While mean performance was consistent, high variance persisted across runs, reinforcing the need for averaging results across seeds when benchmarking deep RL algorithms.

\section{Conclusion}

In the \texttt{MsPacman-v0} environment, Rainbow DQN outperformed PPO in terms of peak reward and long-term potential. While both agents exhibited early learning, Rainbow’s combination of prioritized replay, multi-step returns, and noisy exploration proved more effective in sparse-reward, long-horizon scenarios, despite occasional instability.

PPO showed greater initial stability but suffered from later performance collapse, underscoring its sensitivity to on-policy sampling and hyperparameters. Rainbow's variance highlights its own limitations, yet in our frame budget, it demonstrated superior sample efficiency and strategic discovery.

Future work could enhance both agents by incorporating memory (e.g., LSTM) for improved temporal reasoning, extending training to tens of millions of frames to approach superhuman performance, and doing multiple runs per setting to mitigate training variance. These would strengthen agent robustness and further illuminate the challenges of mastering complex environments like Ms. Pac-Man.

\appendix
\section{Appendix}
\label{sec:appendix}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{PPO_output_hyperparameters_3set.png}
  \caption{PPO Hyperparameters Optimization Results}
  \label{fig:ppo_hyperparams}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\linewidth]{left.jpg}
  \caption{Training performance of PPO agent, to be compared with Rainbow DQN.}
  \label{fig:ppo}
\end{figure}
\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.85\linewidth]{right.png}
  \caption{Training performance of Rainbow DQN agent.}
  \label{fig:rainbow}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{rainbow_8Regular.png}
  \caption{Learning curves for the eight hyperparameter configurations:
           \(\eta\in\{1\times10^{-5},5\times10^{-5}\}\), 
           \(n\in\{3,5\}\), 
           \(\alpha\in\{0.4,0.6\}\).}
  \label{fig:ablation}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\linewidth]{rainbow_8Extended.png}
  \caption{Learning curves for the eight “large” hyperparameter runs (A\_large–H\_large).}
  \label{fig:extended_sweep}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{rainbow_A_seed.png}
  \caption{Configuration A (\(\eta=1\times10^{-5},\,n=3,\,\alpha=0.4\)) across three seeds.}
  \label{fig:seed_A}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\linewidth]{rainbow_E_Seed.png}
  \caption{Configuration E (\(\eta=5\times10^{-5},\,n=3,\,\alpha=0.4\)) across three seeds.}
  \label{fig:seed_E}
\end{figure}

\begin{figure}[htbp]
  \centering
  \includegraphics[width=\linewidth]{rainbow_ablation_smooth.png}
  \caption{Training performance of Rainbow DQN and ablated variants on Ms. Pac-Man.}
  \label{fig:rainbow_ablation}
\end{figure}

\clearpage
\addcontentsline{toc}{section}{References}
\bibliographystyle{plain}
\bibliography{references}

\end{document}
