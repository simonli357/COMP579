{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dae97804",
   "metadata": {},
   "source": [
    "# Policy Gradient Theorem Derivation\n",
    "\n",
    "Consider an MDP with state space $S$, discrete action space $A = \\{a_1, a_2, a_3\\}$, reward function $R$, discount factor $\\gamma$, and a policy parameterized via a softmax function as follows:\n",
    "\n",
    "$$ \n",
    "\\pi(a|s) = \\frac{\\exp(z(s,a))}{\\sum_{a' \\in A} \\exp(z(s,a'))},\n",
    "$$\n",
    "\n",
    "where $z(s,a)$ are the parameters (or scores) corresponding to the state-action pair $(s,a)$. The performance objective is defined as\n",
    "\n",
    "$$ \n",
    "J(\\pi) = \\mathbb{E}_{s \\sim d^\\pi}[V^\\pi(s)],\n",
    "$$\n",
    "\n",
    "with $d^\\pi(s)$ being the steady-state distribution induced by the policy $\\pi$, and the state-value function given by\n",
    "\n",
    "$$ \n",
    "V^\\pi(s) = \\sum_{a \\in A} \\pi(a|s) Q^\\pi(s,a).\n",
    "$$\n",
    "\n",
    "Our goal is to show that the gradient of $J(\\pi)$ with respect to the policy parameter $z(s,a)$ is\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\pi)}{\\partial z(s,a)} = d^\\pi(s) \\; \\pi(a|s) \\; A^\\pi(s,a),\n",
    "$$\n",
    "\n",
    "where the advantage function is defined as\n",
    "\n",
    "$$ \n",
    "A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s).\n",
    "$$\n",
    "\n",
    "## Step 1: Policy Gradient Theorem\n",
    "\n",
    "The policy gradient theorem states that\n",
    "\n",
    "$$ \n",
    "\\nabla_{\\theta} J(\\pi) = \\sum_{s \\in S} d^\\pi(s) \\sum_{a \\in A} \\nabla_{\\theta} \\pi(a|s) \\; Q^\\pi(s,a),\n",
    "$$\n",
    "\n",
    "where $\\theta$ are the parameters of the policy. In our case, $z(s,a)$ serves as these parameters. Using the likelihood ratio trick, we have\n",
    "\n",
    "$$ \n",
    "\\nabla_{\\theta} \\pi(a|s) = \\pi(a|s) \\; \\nabla_{\\theta} \\log \\pi(a|s).\n",
    "$$\n",
    "\n",
    "Substituting this expression into the gradient gives\n",
    "\n",
    "$$ \n",
    "\\nabla_{\\theta} J(\\pi) = \\sum_{s \\in S} d^\\pi(s) \\sum_{a \\in A} \\pi(a|s) \\; \\nabla_{\\theta} \\log \\pi(a|s) \\; Q^\\pi(s,a).\n",
    "$$\n",
    "\n",
    "## Step 2: Differentiating $\\log \\pi(a|s)$ with Respect to $z(s,a)$\n",
    "\n",
    "Since the policy is parameterized as a softmax, we write its log as\n",
    "\n",
    "$$ \n",
    "\\log \\pi(a|s) = z(s,a) - \\log \\Bigl( \\sum_{a' \\in A} \\exp(z(s,a')) \\Bigr).\n",
    "$$\n",
    "\n",
    "Taking the derivative with respect to $z(s,a)$ (for fixed $s$ and $a$):\n",
    "\n",
    "1. The derivative of $z(s,a)$ with respect to itself is $1$.\n",
    "\n",
    "2. For the second term, by the chain rule,\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial z(s,a)} \\log \\Bigl( \\sum_{a' \\in A} \\exp(z(s,a')) \\Bigr) = \\frac{\\exp(z(s,a))}{\\sum_{a' \\in A} \\exp(z(s,a'))} = \\pi(a|s).\n",
    "$$\n",
    "\n",
    "Thus,\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial}{\\partial z(s,a)} \\log \\pi(a|s) = 1 - \\pi(a|s).\n",
    "$$\n",
    "\n",
    "## Step 3: Combining with the Policy Gradient Expression\n",
    "\n",
    "Substitute the derivative back into the gradient expression. Noting that the derivative with respect to $z(s,a)$ is nonzero only for that specific $(s,a)$, we obtain\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\pi)}{\\partial z(s,a)} = d^\\pi(s) \\; \\pi(a|s) \\Bigl[ Q^\\pi(s,a) - \\sum_{a' \\in A} \\pi(a'|s) \\; Q^\\pi(s,a') \\Bigr].\n",
    "$$\n",
    "\n",
    "Since the state-value function is defined as\n",
    "\n",
    "$$ \n",
    "V^\\pi(s) = \\sum_{a' \\in A} \\pi(a'|s) \\; Q^\\pi(s,a'),\n",
    "$$\n",
    "\n",
    "the expression inside the brackets becomes the advantage function\n",
    "\n",
    "$$ \n",
    "A^\\pi(s,a) = Q^\\pi(s,a) - V^\\pi(s).\n",
    "$$\n",
    "\n",
    "Thus, the gradient simplifies to\n",
    "\n",
    "$$ \n",
    "\\frac{\\partial J(\\pi)}{\\partial z(s,a)} = d^\\pi(s) \\; \\pi(a|s) \\; A^\\pi(s,a).\n",
    "$$\n",
    "\n",
    "**Final Result:**\n",
    "\n",
    "$$ \n",
    "\\nabla_z J(\\pi) = \\frac{\\partial J(\\pi)}{\\partial z(s,a)} = d^\\pi(s) \\; \\pi(a|s) \\; A^\\pi(s,a).\n",
    "$$\n",
    "\n",
    "This derivation shows that under the softmax policy parameterization, the gradient of the expected return with respect to the parameter $z(s,a)$ is given by the product of the steady-state distribution $d^\\pi(s)$, the policy probability $\\pi(a|s)$, and the advantage function $A^\\pi(s,a)$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
