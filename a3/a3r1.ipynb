{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import product\n",
    "%matplotlib inline\n",
    "\n",
    "# Neural Network for Q-Value Approximation\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=256, num_layers=2):\n",
    "        super(QNetwork, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for layer in self.net:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                nn.init.uniform_(layer.weight, -0.001, 0.001)\n",
    "                nn.init.uniform_(layer.bias, -0.001, 0.001)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Replay Buffer\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.buffer, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# Training Function\n",
    "def train_agent(env_name, algorithm, use_replay, epsilon, lr, num_episodes=1000, num_seeds=50):\n",
    "    all_rewards = []\n",
    "    for seed in range(num_seeds):\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "        env = gym.make(env_name)\n",
    "        state_dim = env.observation_space.shape[0] if 'Assault' not in env_name else 128\n",
    "        action_dim = env.action_space.n\n",
    "        \n",
    "        # Preprocess state\n",
    "        def preprocess(state):\n",
    "            return state / 255.0 if 'Assault' in env_name else state\n",
    "        \n",
    "        q_net = QNetwork(state_dim, action_dim)\n",
    "        optimizer = optim.Adam(q_net.parameters(), lr=lr)\n",
    "        buffer = ReplayBuffer(1000000) if use_replay else None\n",
    "        \n",
    "        episode_rewards = []\n",
    "        for episode in range(num_episodes):\n",
    "            state = preprocess(env.reset())\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                state_tensor = torch.FloatTensor(state)\n",
    "                with torch.no_grad():\n",
    "                    q_values = q_net(state_tensor)\n",
    "                \n",
    "                # Epsilon-greedy action\n",
    "                if np.random.rand() < epsilon:\n",
    "                    action = env.action_space.sample()\n",
    "                else:\n",
    "                    action = q_values.argmax().item()\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                next_state = preprocess(next_state)\n",
    "                total_reward += reward\n",
    "                \n",
    "                # Store transition\n",
    "                if use_replay:\n",
    "                    buffer.push(state, action, reward, next_state, done)\n",
    "                else:\n",
    "                    # Immediate update\n",
    "                    next_state_tensor = torch.FloatTensor(next_state)\n",
    "                    with torch.no_grad():\n",
    "                        next_q = q_net(next_state_tensor)\n",
    "                    \n",
    "                    if algorithm == 'q_learning':\n",
    "                        target = reward + 0.99 * next_q.max() * (not done)\n",
    "                    elif algorithm == 'expected_sarsa':\n",
    "                        probs = torch.ones(action_dim) * (epsilon / action_dim)\n",
    "                        probs[next_q.argmax()] += (1 - epsilon)\n",
    "                        target = reward + 0.99 * torch.sum(probs * next_q) * (not done)\n",
    "                    \n",
    "                    loss = (q_values[action] - target) ** 2\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                # Replay buffer update\n",
    "                if use_replay and len(buffer) >= 64:\n",
    "                    batch = buffer.sample(64)\n",
    "                    states, actions, rewards, next_states, dones = zip(*batch)\n",
    "                    \n",
    "                    states = torch.FloatTensor(np.array(states))\n",
    "                    actions = torch.LongTensor(actions)\n",
    "                    rewards = torch.FloatTensor(rewards)\n",
    "                    next_states = torch.FloatTensor(np.array(next_states))\n",
    "                    dones = torch.BoolTensor(dones)\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        next_q = q_net(next_states)\n",
    "                        if algorithm == 'q_learning':\n",
    "                            targets = rewards + 0.99 * next_q.max(1)[0] * (~dones)\n",
    "                        elif algorithm == 'expected_sarsa':\n",
    "                            probs = torch.ones_like(next_q) * (epsilon / action_dim)\n",
    "                            best_actions = next_q.argmax(1)\n",
    "                            probs[range(64), best_actions] += (1 - epsilon)\n",
    "                            targets = rewards + 0.99 * (probs * next_q).sum(1) * (~dones)\n",
    "                    \n",
    "                    current_q = q_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "                    loss = nn.functional.mse_loss(current_q, targets)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                state = next_state\n",
    "            \n",
    "            episode_rewards.append(total_reward)\n",
    "            print(f'Seed {seed}, Episode {episode}: Reward {total_reward}')\n",
    "        \n",
    "        all_rewards.append(episode_rewards)\n",
    "        env.close()\n",
    "    \n",
    "    return np.array(all_rewards)\n",
    "\n",
    "# Plotting Results\n",
    "def plot_curves(env_name, use_replay, data):\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    line_styles = ['-', '--', ':']\n",
    "    for i, (algo, eps, lr) in enumerate(data.keys()):\n",
    "        rewards = data[(algo, eps, lr)]\n",
    "        mean = np.mean(rewards, axis=0)\n",
    "        std = np.std(rewards, axis=0)\n",
    "        x = np.arange(len(mean))\n",
    "        color = 'green' if algo == 'q_learning' else 'red'\n",
    "        ls = line_styles[int(np.log2(lr)) - 2]  # lr 1/4, 1/8, 1/16\n",
    "        plt.plot(x, mean, color=color, linestyle=ls, label=f'{algo}, ε={eps}, lr={lr}')\n",
    "        plt.fill_between(x, mean - std, mean + std, color=color, alpha=0.1)\n",
    "    \n",
    "    plt.title(f'{env_name} {\"with\" if use_replay else \"without\"} Replay Buffer')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.savefig(f'{env_name}_{\"replay\" if use_replay else \"no_replay\"}.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Acrobot-v1 q_learning ε=0.1 lr=0.25 buffer=True\n",
      "Seed 0, Episode 0: Reward -500.0\n",
      "Seed 0, Episode 1: Reward -500.0\n",
      "Seed 0, Episode 2: Reward -500.0\n",
      "Seed 0, Episode 3: Reward -500.0\n",
      "Seed 0, Episode 4: Reward -500.0\n",
      "Seed 0, Episode 5: Reward -500.0\n",
      "Seed 0, Episode 6: Reward -500.0\n",
      "Seed 0, Episode 7: Reward -500.0\n",
      "Seed 0, Episode 8: Reward -500.0\n",
      "Seed 0, Episode 9: Reward -500.0\n",
      "Seed 0, Episode 10: Reward -500.0\n",
      "Seed 0, Episode 11: Reward -500.0\n",
      "Seed 0, Episode 12: Reward -500.0\n",
      "Seed 0, Episode 13: Reward -500.0\n",
      "Seed 0, Episode 14: Reward -382.0\n",
      "Seed 0, Episode 15: Reward -189.0\n",
      "Seed 0, Episode 16: Reward -500.0\n",
      "Seed 0, Episode 17: Reward -500.0\n",
      "Seed 0, Episode 18: Reward -500.0\n",
      "Seed 0, Episode 19: Reward -500.0\n",
      "Seed 0, Episode 20: Reward -500.0\n",
      "Seed 0, Episode 21: Reward -500.0\n",
      "Seed 0, Episode 22: Reward -500.0\n",
      "Seed 0, Episode 23: Reward -451.0\n",
      "Seed 0, Episode 24: Reward -500.0\n",
      "Seed 0, Episode 25: Reward -500.0\n",
      "Seed 0, Episode 26: Reward -500.0\n",
      "Seed 0, Episode 27: Reward -346.0\n",
      "Seed 0, Episode 28: Reward -399.0\n",
      "Seed 0, Episode 29: Reward -500.0\n",
      "Seed 0, Episode 30: Reward -500.0\n",
      "Seed 0, Episode 31: Reward -500.0\n",
      "Seed 0, Episode 32: Reward -500.0\n",
      "Seed 0, Episode 33: Reward -500.0\n",
      "Seed 0, Episode 34: Reward -500.0\n",
      "Seed 0, Episode 35: Reward -500.0\n",
      "Seed 0, Episode 36: Reward -500.0\n",
      "Seed 0, Episode 37: Reward -500.0\n",
      "Seed 0, Episode 38: Reward -500.0\n",
      "Seed 0, Episode 39: Reward -500.0\n",
      "Seed 0, Episode 40: Reward -500.0\n",
      "Seed 0, Episode 41: Reward -500.0\n",
      "Seed 0, Episode 42: Reward -500.0\n",
      "Seed 0, Episode 43: Reward -500.0\n",
      "Seed 0, Episode 44: Reward -500.0\n",
      "Seed 0, Episode 45: Reward -500.0\n",
      "Seed 0, Episode 46: Reward -500.0\n",
      "Seed 0, Episode 47: Reward -500.0\n",
      "Seed 0, Episode 48: Reward -500.0\n",
      "Seed 0, Episode 49: Reward -500.0\n",
      "Seed 0, Episode 50: Reward -500.0\n",
      "Seed 0, Episode 51: Reward -500.0\n",
      "Seed 0, Episode 52: Reward -500.0\n",
      "Seed 0, Episode 53: Reward -250.0\n",
      "Seed 0, Episode 54: Reward -500.0\n",
      "Seed 0, Episode 55: Reward -500.0\n",
      "Seed 0, Episode 56: Reward -500.0\n",
      "Seed 0, Episode 57: Reward -500.0\n",
      "Seed 0, Episode 58: Reward -234.0\n",
      "Seed 0, Episode 59: Reward -104.0\n",
      "Seed 0, Episode 60: Reward -178.0\n",
      "Seed 0, Episode 61: Reward -182.0\n",
      "Seed 0, Episode 62: Reward -195.0\n",
      "Seed 0, Episode 63: Reward -142.0\n",
      "Seed 0, Episode 64: Reward -500.0\n",
      "Seed 0, Episode 65: Reward -500.0\n",
      "Seed 0, Episode 66: Reward -354.0\n",
      "Seed 0, Episode 67: Reward -253.0\n",
      "Seed 0, Episode 68: Reward -429.0\n",
      "Seed 0, Episode 69: Reward -500.0\n",
      "Seed 0, Episode 70: Reward -406.0\n",
      "Seed 0, Episode 71: Reward -310.0\n",
      "Seed 0, Episode 72: Reward -224.0\n",
      "Seed 0, Episode 73: Reward -286.0\n",
      "Seed 0, Episode 74: Reward -312.0\n",
      "Seed 0, Episode 75: Reward -314.0\n",
      "Seed 0, Episode 76: Reward -256.0\n",
      "Seed 0, Episode 77: Reward -204.0\n",
      "Seed 0, Episode 78: Reward -140.0\n",
      "Seed 0, Episode 79: Reward -500.0\n",
      "Seed 0, Episode 80: Reward -298.0\n",
      "Seed 0, Episode 81: Reward -500.0\n",
      "Seed 0, Episode 82: Reward -156.0\n",
      "Seed 0, Episode 83: Reward -266.0\n",
      "Seed 0, Episode 84: Reward -125.0\n",
      "Seed 0, Episode 85: Reward -500.0\n",
      "Seed 0, Episode 86: Reward -500.0\n",
      "Seed 0, Episode 87: Reward -500.0\n",
      "Seed 0, Episode 88: Reward -500.0\n",
      "Seed 0, Episode 89: Reward -500.0\n",
      "Seed 0, Episode 90: Reward -500.0\n",
      "Seed 0, Episode 91: Reward -405.0\n",
      "Seed 0, Episode 92: Reward -496.0\n",
      "Seed 0, Episode 93: Reward -317.0\n",
      "Seed 0, Episode 94: Reward -500.0\n",
      "Seed 0, Episode 95: Reward -500.0\n",
      "Seed 0, Episode 96: Reward -232.0\n",
      "Seed 0, Episode 97: Reward -411.0\n",
      "Seed 0, Episode 98: Reward -202.0\n",
      "Seed 0, Episode 99: Reward -130.0\n",
      "Seed 0, Episode 100: Reward -157.0\n",
      "Seed 0, Episode 101: Reward -179.0\n",
      "Seed 0, Episode 102: Reward -163.0\n",
      "Seed 0, Episode 103: Reward -163.0\n",
      "Seed 0, Episode 104: Reward -136.0\n",
      "Seed 0, Episode 105: Reward -211.0\n",
      "Seed 0, Episode 106: Reward -114.0\n",
      "Seed 0, Episode 107: Reward -153.0\n",
      "Seed 0, Episode 108: Reward -167.0\n",
      "Seed 0, Episode 109: Reward -180.0\n",
      "Seed 0, Episode 110: Reward -161.0\n",
      "Seed 0, Episode 111: Reward -161.0\n",
      "Seed 0, Episode 112: Reward -158.0\n",
      "Seed 0, Episode 113: Reward -145.0\n",
      "Seed 0, Episode 114: Reward -174.0\n",
      "Seed 0, Episode 115: Reward -291.0\n",
      "Seed 0, Episode 116: Reward -229.0\n",
      "Seed 0, Episode 117: Reward -157.0\n",
      "Seed 0, Episode 118: Reward -132.0\n",
      "Seed 0, Episode 119: Reward -134.0\n",
      "Seed 0, Episode 120: Reward -314.0\n",
      "Seed 0, Episode 121: Reward -240.0\n",
      "Seed 0, Episode 122: Reward -180.0\n",
      "Seed 0, Episode 123: Reward -171.0\n",
      "Seed 0, Episode 124: Reward -192.0\n",
      "Seed 0, Episode 125: Reward -234.0\n",
      "Seed 0, Episode 126: Reward -401.0\n",
      "Seed 0, Episode 127: Reward -226.0\n",
      "Seed 0, Episode 128: Reward -362.0\n",
      "Seed 0, Episode 129: Reward -500.0\n",
      "Seed 0, Episode 130: Reward -500.0\n",
      "Seed 0, Episode 131: Reward -500.0\n",
      "Seed 0, Episode 132: Reward -500.0\n",
      "Seed 0, Episode 133: Reward -500.0\n",
      "Seed 0, Episode 134: Reward -500.0\n",
      "Seed 0, Episode 135: Reward -500.0\n",
      "Seed 0, Episode 136: Reward -500.0\n",
      "Seed 0, Episode 137: Reward -500.0\n",
      "Seed 0, Episode 138: Reward -500.0\n",
      "Seed 0, Episode 139: Reward -500.0\n",
      "Seed 0, Episode 140: Reward -314.0\n",
      "Seed 0, Episode 141: Reward -500.0\n",
      "Seed 0, Episode 142: Reward -500.0\n",
      "Seed 0, Episode 143: Reward -500.0\n",
      "Seed 0, Episode 144: Reward -500.0\n",
      "Seed 0, Episode 145: Reward -500.0\n",
      "Seed 0, Episode 146: Reward -500.0\n",
      "Seed 0, Episode 147: Reward -500.0\n",
      "Seed 0, Episode 148: Reward -500.0\n",
      "Seed 0, Episode 149: Reward -500.0\n",
      "Seed 0, Episode 150: Reward -500.0\n",
      "Seed 0, Episode 151: Reward -500.0\n",
      "Seed 0, Episode 152: Reward -500.0\n",
      "Seed 0, Episode 153: Reward -500.0\n",
      "Seed 0, Episode 154: Reward -500.0\n",
      "Seed 0, Episode 155: Reward -500.0\n",
      "Seed 0, Episode 156: Reward -500.0\n",
      "Seed 0, Episode 157: Reward -500.0\n",
      "Seed 0, Episode 158: Reward -500.0\n",
      "Seed 0, Episode 159: Reward -500.0\n",
      "Seed 0, Episode 160: Reward -500.0\n",
      "Seed 0, Episode 161: Reward -500.0\n",
      "Seed 0, Episode 162: Reward -500.0\n",
      "Seed 0, Episode 163: Reward -500.0\n",
      "Seed 0, Episode 164: Reward -500.0\n",
      "Seed 0, Episode 165: Reward -500.0\n",
      "Seed 0, Episode 166: Reward -500.0\n",
      "Seed 0, Episode 167: Reward -500.0\n",
      "Seed 0, Episode 168: Reward -500.0\n",
      "Seed 0, Episode 169: Reward -500.0\n",
      "Seed 0, Episode 170: Reward -500.0\n",
      "Seed 0, Episode 171: Reward -500.0\n",
      "Seed 0, Episode 172: Reward -446.0\n",
      "Seed 0, Episode 173: Reward -500.0\n",
      "Seed 0, Episode 174: Reward -500.0\n",
      "Seed 0, Episode 175: Reward -415.0\n",
      "Seed 0, Episode 176: Reward -471.0\n",
      "Seed 0, Episode 177: Reward -185.0\n",
      "Seed 0, Episode 178: Reward -500.0\n",
      "Seed 0, Episode 179: Reward -316.0\n",
      "Seed 0, Episode 180: Reward -196.0\n",
      "Seed 0, Episode 181: Reward -413.0\n",
      "Seed 0, Episode 182: Reward -340.0\n",
      "Seed 0, Episode 183: Reward -500.0\n",
      "Seed 0, Episode 184: Reward -500.0\n",
      "Seed 0, Episode 185: Reward -500.0\n",
      "Seed 0, Episode 186: Reward -500.0\n",
      "Seed 0, Episode 187: Reward -500.0\n",
      "Seed 0, Episode 188: Reward -500.0\n",
      "Seed 0, Episode 189: Reward -500.0\n",
      "Seed 0, Episode 190: Reward -500.0\n",
      "Seed 0, Episode 191: Reward -500.0\n",
      "Seed 0, Episode 192: Reward -500.0\n",
      "Seed 0, Episode 193: Reward -500.0\n",
      "Seed 0, Episode 194: Reward -500.0\n",
      "Seed 0, Episode 195: Reward -500.0\n",
      "Seed 0, Episode 196: Reward -500.0\n",
      "Seed 0, Episode 197: Reward -500.0\n",
      "Seed 0, Episode 198: Reward -500.0\n",
      "Seed 0, Episode 199: Reward -500.0\n",
      "Seed 0, Episode 200: Reward -500.0\n",
      "Seed 0, Episode 201: Reward -500.0\n",
      "Seed 0, Episode 202: Reward -500.0\n",
      "Seed 0, Episode 203: Reward -500.0\n",
      "Seed 0, Episode 204: Reward -500.0\n",
      "Seed 0, Episode 205: Reward -500.0\n",
      "Seed 0, Episode 206: Reward -500.0\n",
      "Seed 0, Episode 207: Reward -500.0\n",
      "Seed 0, Episode 208: Reward -500.0\n",
      "Seed 0, Episode 209: Reward -500.0\n",
      "Seed 0, Episode 210: Reward -500.0\n",
      "Seed 0, Episode 211: Reward -500.0\n",
      "Seed 0, Episode 212: Reward -500.0\n",
      "Seed 0, Episode 213: Reward -500.0\n",
      "Seed 0, Episode 214: Reward -500.0\n",
      "Seed 0, Episode 215: Reward -500.0\n",
      "Seed 0, Episode 216: Reward -500.0\n",
      "Seed 0, Episode 217: Reward -500.0\n",
      "Seed 0, Episode 218: Reward -500.0\n",
      "Seed 0, Episode 219: Reward -422.0\n",
      "Seed 0, Episode 220: Reward -500.0\n",
      "Seed 0, Episode 221: Reward -500.0\n",
      "Seed 0, Episode 222: Reward -500.0\n",
      "Seed 0, Episode 223: Reward -500.0\n",
      "Seed 0, Episode 224: Reward -483.0\n",
      "Seed 0, Episode 225: Reward -377.0\n",
      "Seed 0, Episode 226: Reward -500.0\n",
      "Seed 0, Episode 227: Reward -386.0\n",
      "Seed 0, Episode 228: Reward -500.0\n",
      "Seed 0, Episode 229: Reward -500.0\n",
      "Seed 0, Episode 230: Reward -500.0\n",
      "Seed 0, Episode 231: Reward -334.0\n",
      "Seed 0, Episode 232: Reward -500.0\n",
      "Seed 0, Episode 233: Reward -500.0\n",
      "Seed 0, Episode 234: Reward -500.0\n",
      "Seed 0, Episode 235: Reward -184.0\n",
      "Seed 0, Episode 236: Reward -257.0\n",
      "Seed 0, Episode 237: Reward -500.0\n",
      "Seed 0, Episode 238: Reward -473.0\n",
      "Seed 0, Episode 239: Reward -500.0\n",
      "Seed 0, Episode 240: Reward -500.0\n",
      "Seed 0, Episode 241: Reward -500.0\n",
      "Seed 0, Episode 242: Reward -500.0\n",
      "Seed 0, Episode 243: Reward -500.0\n",
      "Seed 0, Episode 244: Reward -500.0\n",
      "Seed 0, Episode 245: Reward -500.0\n",
      "Seed 0, Episode 246: Reward -500.0\n",
      "Seed 0, Episode 247: Reward -500.0\n",
      "Seed 0, Episode 248: Reward -500.0\n",
      "Seed 0, Episode 249: Reward -500.0\n",
      "Seed 0, Episode 250: Reward -500.0\n",
      "Seed 0, Episode 251: Reward -500.0\n",
      "Seed 0, Episode 252: Reward -500.0\n",
      "Seed 0, Episode 253: Reward -500.0\n",
      "Seed 0, Episode 254: Reward -500.0\n",
      "Seed 0, Episode 255: Reward -500.0\n",
      "Seed 0, Episode 256: Reward -500.0\n",
      "Seed 0, Episode 257: Reward -500.0\n",
      "Seed 0, Episode 258: Reward -500.0\n",
      "Seed 0, Episode 259: Reward -500.0\n",
      "Seed 0, Episode 260: Reward -500.0\n",
      "Seed 0, Episode 261: Reward -500.0\n",
      "Seed 0, Episode 262: Reward -133.0\n",
      "Seed 0, Episode 263: Reward -500.0\n",
      "Seed 0, Episode 264: Reward -500.0\n",
      "Seed 0, Episode 265: Reward -500.0\n",
      "Seed 0, Episode 266: Reward -500.0\n",
      "Seed 0, Episode 267: Reward -500.0\n",
      "Seed 0, Episode 268: Reward -500.0\n",
      "Seed 0, Episode 269: Reward -500.0\n",
      "Seed 0, Episode 270: Reward -500.0\n",
      "Seed 0, Episode 271: Reward -500.0\n",
      "Seed 0, Episode 272: Reward -500.0\n",
      "Seed 0, Episode 273: Reward -500.0\n",
      "Seed 0, Episode 274: Reward -500.0\n",
      "Seed 0, Episode 275: Reward -438.0\n",
      "Seed 0, Episode 276: Reward -500.0\n",
      "Seed 0, Episode 277: Reward -500.0\n",
      "Seed 0, Episode 278: Reward -138.0\n",
      "Seed 0, Episode 279: Reward -500.0\n",
      "Seed 0, Episode 280: Reward -500.0\n",
      "Seed 0, Episode 281: Reward -500.0\n",
      "Seed 0, Episode 282: Reward -413.0\n",
      "Seed 0, Episode 283: Reward -500.0\n",
      "Seed 0, Episode 284: Reward -500.0\n",
      "Seed 0, Episode 285: Reward -500.0\n",
      "Seed 0, Episode 286: Reward -500.0\n",
      "Seed 0, Episode 287: Reward -500.0\n",
      "Seed 0, Episode 288: Reward -500.0\n",
      "Seed 0, Episode 289: Reward -500.0\n",
      "Seed 0, Episode 290: Reward -500.0\n",
      "Seed 0, Episode 291: Reward -500.0\n",
      "Seed 0, Episode 292: Reward -500.0\n",
      "Seed 0, Episode 293: Reward -500.0\n",
      "Seed 0, Episode 294: Reward -500.0\n",
      "Seed 0, Episode 295: Reward -500.0\n",
      "Seed 0, Episode 296: Reward -500.0\n",
      "Seed 0, Episode 297: Reward -500.0\n",
      "Seed 0, Episode 298: Reward -500.0\n",
      "Seed 0, Episode 299: Reward -500.0\n",
      "Seed 0, Episode 300: Reward -500.0\n",
      "Seed 0, Episode 301: Reward -500.0\n",
      "Seed 0, Episode 302: Reward -500.0\n",
      "Seed 0, Episode 303: Reward -500.0\n",
      "Seed 0, Episode 304: Reward -500.0\n",
      "Seed 0, Episode 305: Reward -500.0\n",
      "Seed 0, Episode 306: Reward -500.0\n",
      "Seed 0, Episode 307: Reward -500.0\n",
      "Seed 0, Episode 308: Reward -500.0\n",
      "Seed 0, Episode 309: Reward -500.0\n",
      "Seed 0, Episode 310: Reward -500.0\n",
      "Seed 0, Episode 311: Reward -500.0\n",
      "Seed 0, Episode 312: Reward -500.0\n",
      "Seed 0, Episode 313: Reward -500.0\n",
      "Seed 0, Episode 314: Reward -500.0\n",
      "Seed 0, Episode 315: Reward -500.0\n",
      "Seed 0, Episode 316: Reward -500.0\n",
      "Seed 0, Episode 317: Reward -500.0\n",
      "Seed 0, Episode 318: Reward -500.0\n",
      "Seed 0, Episode 319: Reward -500.0\n",
      "Seed 0, Episode 320: Reward -500.0\n",
      "Seed 0, Episode 321: Reward -500.0\n",
      "Seed 0, Episode 322: Reward -500.0\n",
      "Seed 0, Episode 323: Reward -500.0\n",
      "Seed 0, Episode 324: Reward -500.0\n",
      "Seed 0, Episode 325: Reward -500.0\n",
      "Seed 0, Episode 326: Reward -500.0\n",
      "Seed 0, Episode 327: Reward -500.0\n",
      "Seed 0, Episode 328: Reward -500.0\n",
      "Seed 0, Episode 329: Reward -500.0\n",
      "Seed 0, Episode 330: Reward -500.0\n",
      "Seed 0, Episode 331: Reward -500.0\n",
      "Seed 0, Episode 332: Reward -500.0\n",
      "Seed 0, Episode 333: Reward -500.0\n",
      "Seed 0, Episode 334: Reward -500.0\n",
      "Seed 0, Episode 335: Reward -500.0\n",
      "Seed 0, Episode 336: Reward -500.0\n",
      "Seed 0, Episode 337: Reward -274.0\n",
      "Seed 0, Episode 338: Reward -500.0\n",
      "Seed 0, Episode 339: Reward -500.0\n",
      "Seed 0, Episode 340: Reward -500.0\n",
      "Seed 0, Episode 341: Reward -500.0\n",
      "Seed 0, Episode 342: Reward -500.0\n",
      "Seed 0, Episode 343: Reward -500.0\n",
      "Seed 0, Episode 344: Reward -479.0\n",
      "Seed 0, Episode 345: Reward -500.0\n",
      "Seed 0, Episode 346: Reward -310.0\n",
      "Seed 0, Episode 347: Reward -500.0\n",
      "Seed 0, Episode 348: Reward -500.0\n",
      "Seed 0, Episode 349: Reward -500.0\n",
      "Seed 0, Episode 350: Reward -500.0\n",
      "Seed 0, Episode 351: Reward -500.0\n",
      "Seed 0, Episode 352: Reward -438.0\n",
      "Seed 0, Episode 353: Reward -500.0\n",
      "Seed 0, Episode 354: Reward -500.0\n",
      "Seed 0, Episode 355: Reward -500.0\n",
      "Seed 0, Episode 356: Reward -500.0\n",
      "Seed 0, Episode 357: Reward -500.0\n",
      "Seed 0, Episode 358: Reward -500.0\n",
      "Seed 0, Episode 359: Reward -500.0\n",
      "Seed 0, Episode 360: Reward -500.0\n",
      "Seed 0, Episode 361: Reward -500.0\n",
      "Seed 0, Episode 362: Reward -500.0\n",
      "Seed 0, Episode 363: Reward -500.0\n",
      "Seed 0, Episode 364: Reward -500.0\n",
      "Seed 0, Episode 365: Reward -500.0\n",
      "Seed 0, Episode 366: Reward -500.0\n",
      "Seed 0, Episode 367: Reward -500.0\n",
      "Seed 0, Episode 368: Reward -500.0\n",
      "Seed 0, Episode 369: Reward -500.0\n",
      "Seed 0, Episode 370: Reward -500.0\n",
      "Seed 0, Episode 371: Reward -500.0\n",
      "Seed 0, Episode 372: Reward -353.0\n",
      "Seed 0, Episode 373: Reward -500.0\n",
      "Seed 0, Episode 374: Reward -500.0\n",
      "Seed 0, Episode 375: Reward -500.0\n",
      "Seed 0, Episode 376: Reward -487.0\n",
      "Seed 0, Episode 377: Reward -300.0\n",
      "Seed 0, Episode 378: Reward -500.0\n",
      "Seed 0, Episode 379: Reward -500.0\n",
      "Seed 0, Episode 380: Reward -500.0\n",
      "Seed 0, Episode 381: Reward -500.0\n",
      "Seed 0, Episode 382: Reward -500.0\n",
      "Seed 0, Episode 383: Reward -500.0\n",
      "Seed 0, Episode 384: Reward -500.0\n",
      "Seed 0, Episode 385: Reward -500.0\n",
      "Seed 0, Episode 386: Reward -500.0\n",
      "Seed 0, Episode 387: Reward -500.0\n",
      "Seed 0, Episode 388: Reward -500.0\n",
      "Seed 0, Episode 389: Reward -500.0\n",
      "Seed 0, Episode 390: Reward -500.0\n",
      "Seed 0, Episode 391: Reward -500.0\n",
      "Seed 0, Episode 392: Reward -500.0\n",
      "Seed 0, Episode 393: Reward -500.0\n",
      "Seed 0, Episode 394: Reward -500.0\n",
      "Seed 0, Episode 395: Reward -500.0\n",
      "Seed 0, Episode 396: Reward -500.0\n",
      "Seed 0, Episode 397: Reward -500.0\n",
      "Seed 0, Episode 398: Reward -500.0\n",
      "Seed 0, Episode 399: Reward -484.0\n",
      "Seed 0, Episode 400: Reward -500.0\n",
      "Seed 0, Episode 401: Reward -500.0\n",
      "Seed 0, Episode 402: Reward -500.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m algo, eps, lr \u001b[38;5;129;01min\u001b[39;00m product(algorithms, epsilons, lrs):\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00malgo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ε=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00meps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m lr=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m buffer=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00muse_replay\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     rewards \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malgo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_replay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     results[(algo, eps, lr)] \u001b[38;5;241m=\u001b[39m rewards\n\u001b[1;32m     14\u001b[0m plot_curves(env, use_replay, results)\n",
      "Cell \u001b[0;32mIn[2], line 137\u001b[0m, in \u001b[0;36mtrain_agent\u001b[0;34m(env_name, algorithm, use_replay, epsilon, lr, num_episodes, num_seeds)\u001b[0m\n\u001b[1;32m    135\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    136\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m--> 137\u001b[0m         \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     state \u001b[38;5;241m=\u001b[39m next_state\n\u001b[1;32m    141\u001b[0m episode_rewards\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py:28\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m():\n\u001b[0;32m---> 28\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py:107\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[1;32m    105\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m--> 107\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py:94\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     denom \u001b[38;5;241m=\u001b[39m (\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(bias_correction2))\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[1;32m     96\u001b[0m step_size \u001b[38;5;241m=\u001b[39m lr \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m     98\u001b[0m param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "envs = ['Acrobot-v1', 'ALE/Assault-ram-v5']\n",
    "algorithms = ['q_learning', 'expected_sarsa']\n",
    "epsilons = [0.1, 0.3, 0.5]\n",
    "lrs = [0.25, 0.125, 0.0625]\n",
    "buffers = [True, False]\n",
    "\n",
    "for env in envs:\n",
    "    for use_replay in buffers:\n",
    "        results = {}\n",
    "        for algo, eps, lr in product(algorithms, epsilons, lrs):\n",
    "            print(f'Training {env} {algo} ε={eps} lr={lr} buffer={use_replay}')\n",
    "            rewards = train_agent(env, algo, use_replay, eps, lr, 1000, 50)\n",
    "            results[(algo, eps, lr)] = rewards\n",
    "        plot_curves(env, use_replay, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
